# -*- coding: utf-8 -*-
"""AML Group Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ll2dTEJ-XaUUxowxQUKNLcF_UQP2lC_5

#**Introduction**

This project builds on our previous work predicting day-ahead electricity prices for Norway’s NO2 zone. This time, we're taking it further—adding more detailed weather data for a more realistic view and testing advanced deep learning models like **RNN, LSTM, GRU, and TCN**. The dataset now covers to March 2025, giving a longer timeframe for better predictions.

#**Data Collection**
Data is collected using the **Open-Meteo and ENTSO-E API**.

The following datasets are retrieved:

1.   **Day-Ahead Prices**: Historical day-ahead electricity prices for the NO2 zone.
2.   **Load Forecasts**: Forecasted electricity demand for multiple areas, including NO2 and neighboring regions.
3.   **Wind and Solar Forecasts**: Expected renewable energy production, which directly impacts electricity supply and pricing.

1.   **Net Transfer Capacities (NTC)**: Week-ahead net transfer capacities for specific cross-border connections.
1.   **Weather Data**:
Temperature, Wind Speed, Cloud Cover: These factors influence both electricity generation (e.g., wind and solar output) and demand (e.g., heating or cooling needs).
1.   **Water Reservoir Levels**:
A critical factor for Norway’s hydro-based power generation, impacting supply stability and prices.

# Install Package if Needed
"""

!pip install entsoe-py
!pip install openmeteo-requests
!pip install requests-cache retry-requests numpy pandas
!pip install ydata-profiling

"""# For Open-Meteo Data Refreshing"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import openmeteo_requests
import requests_cache
from retry_requests import retry
from entsoe import EntsoePandasClient


#Open-Meteo data
start = pd.Timestamp('2023-10-01', tz='Europe/Brussels')
end = pd.Timestamp('2025-03-30', tz='Europe/Brussels')
extended_start = start - pd.Timedelta(days=1)

#ENTSOe data
client = EntsoePandasClient(api_key='d43c0033-144a-4c29-aa12-98d6d1070332')
start1 = pd.Timestamp('20231001', tz='Europe/Brussels')
end1 = pd.Timestamp('20250330', tz='Europe/Brussels')
extended_start1 = start1 - pd.Timedelta(days=1)

# Weather API Setup
cache_session = requests_cache.CachedSession('.cache', expire_after=3600)
retry_session = retry(cache_session, retries=5, backoff_factor=0.2)
openmeteo = openmeteo_requests.Client(session=retry_session)

# Multiple Locations (Oslo for NO_1, Kristiansand for NO_2, Bergen for NO_5, Aalborg for DK, Rotterdam for NL and Kiel for DE_LU.)
locations = {
    "NO_1": (59.9127, 10.7461),
    "NO_2": (58.1467, 7.9956),
    "NO_5": (60.393, 5.3242),
    "DK": (57.048, 9.9187),
    "NL": (51.9225, 4.4792),
    "DE_LU": (54.3213, 10.1349),
}

# Function to Get Weather Data
def fetch_weather_data(city, lat, lon, start_date, end_date):
    url = "https://historical-forecast-api.open-meteo.com/v1/forecast"
    params = {
        "latitude": lat,
        "longitude": lon,
        "start_date": start_date.strftime("%Y-%m-%d"),
        "end_date": end_date.strftime("%Y-%m-%d"),
        "hourly": ["temperature_2m", "wind_speed_80m", "cloud_cover"]
    }

    responses = openmeteo.weather_api(url, params=params)
    response = responses[0]  # Single location
    hourly = response.Hourly()

    weather_data = {
        "timestamp": pd.date_range(
            start=pd.to_datetime(hourly.Time(), unit="s", utc=True),
            end=pd.to_datetime(hourly.TimeEnd(), unit="s", utc=True),
            freq=pd.Timedelta(seconds=hourly.Interval()),
            inclusive="left"
        ),
        f"temperature_2m_{city}": hourly.Variables(0).ValuesAsNumpy(),
        f"wind_speed_80m_{city}": hourly.Variables(1).ValuesAsNumpy(),
        f"cloud_cover_{city}": hourly.Variables(2).ValuesAsNumpy(),
    }

    return pd.DataFrame(weather_data)

# Get Weather Data for All Locations
weather_df = []
for city, (lat, lon) in locations.items():
    df = fetch_weather_data(city, lat, lon, start, end)
    df["timestamp"] = df["timestamp"].dt.tz_convert("Europe/Brussels")
    weather_df.append(df)

weather_df = pd.concat(weather_df, axis=1)
weather_df = weather_df.loc[:,~weather_df.columns.duplicated()]

"""# For ENTSO-E Data Refreshing"""

#ENTSO-E

# Get Day-Ahead Prices for NO_2
try:
    da_prices = client.query_day_ahead_prices("NO_2", start=extended_start1, end=end1).to_frame(name='DA_prices_NO_2')
    da_prices['Prev_Day_DA_prices_NO_2'] = da_prices['DA_prices_NO_2'].shift(1)
    da_prices = da_prices.loc[start:end]  # Remove extended day
except Exception as e:
    print(f"No data: {e}")

# Get Load Forecast for multiple areas
areas_load_forecast = ["NO_2", "NO_1", "NO_5", "DK", "NL", "DE_LU"]
load_forecast_dfs = []
for area in areas_load_forecast:
    try:
        data = client.query_load_forecast(area, start=start1, end=end1).rename(columns={'Forecasted Load': f'Load_forecast_{area}'})
        load_forecast_dfs.append(data)
    except Exception as e:
        print(f"No data Load Forecast for: {e}")


load_forecast_df = pd.concat(load_forecast_dfs, axis=1)

# Get Wind and Solar Forecast for multiple areas
areas_wind_and_solar_forecast = areas_load_forecast  # Same as load forecast areas
wind_solar_forecast_dfs =[]
for area in areas_wind_and_solar_forecast:
    try:
        data = client.query_wind_and_solar_forecast(area, start=start1, end=end1)
        data.columns = [f"{col}_{area}" for col in data.columns]
        wind_solar_forecast_dfs.append(data)
    except Exception as e:
        print(f"No data for Wind and Solar Forecast in {area}: {e}")

# Combine Wind and Solar Forecast Data
wind_solar_forecast_df = pd.concat(wind_solar_forecast_dfs, axis=1)

# Get Net Transfer Capacity (Week-Ahead) for specified pairs
ntc_pairs = [
    ("NO_2", "NL"),
    ("NO_2", "DK"),
    ("NO_2", "DE_LU"),
    ("NO_2", "GB"),
    ("NO_2", "NO_5")
]

ntc_dfs = []
for from_area, to_area in ntc_pairs:
    try:
        data = client.query_net_transfer_capacity_weekahead(from_area, to_area, start=start1, end=end1).to_frame(
            name=f'NTC_WeekAhead_{from_area}_to_{to_area}')
        ntc_dfs.append(data)
    except Exception as e:
        print(f"No data Net Transfer Capacity {from_area} to {to_area}: {e}")


ntc_df = pd.concat(ntc_dfs, axis=1)

# Get Aggregate Water Reservoirs and Hydro Storage for NO_2
try:
    water_reservoirs = client.query_aggregate_water_reservoirs_and_hydro_storage("NO_2", start=start1, end=end1).to_frame(name='Water_Reservoirs_NO_2')
except Exception as e:
    print(f"No data Water Reservoirs for NO_2: {e}")

"""# Save Data for Later Use"""

weather_df.to_csv("weather_energy_data.csv")
da_prices.to_csv("da_prices.csv")
load_forecast_df.to_csv("load_forecast_df.csv")
wind_solar_forecast_df.to_csv("wind_solar_forecast_df.csv")
ntc_df.to_csv("ntc_df.csv")
water_reservoirs.to_csv("water_reservoirs.csv")

"""# Load Data"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

weather_df = pd.read_csv("weather_energy_data.csv", index_col=0, parse_dates=True)
da_prices = pd.read_csv("da_prices.csv", index_col=0, parse_dates=True)
load_forecast_df = pd.read_csv("load_forecast_df.csv", index_col=0, parse_dates=True)
wind_solar_forecast_df = pd.read_csv("wind_solar_forecast_df.csv", index_col=0, parse_dates=True)
ntc_df = pd.read_csv("ntc_df.csv", index_col=0, parse_dates=True)
water_reservoirs = pd.read_csv("water_reservoirs.csv", index_col=0, parse_dates=True)

# Ensure same time range (start to end)
common_time_range = pd.date_range(start=start1, end=end1, freq="H")

# Reindex all DataFrames to ensure consistency
da_prices = da_prices.reindex(common_time_range)
weather_df = weather_df.reindex(common_time_range)

load_forecast_df = load_forecast_df.reindex(common_time_range)
ntc_df = ntc_df.reindex(common_time_range)
water_reservoirs = water_reservoirs.reindex(common_time_range)
wind_solar_forecast_df = wind_solar_forecast_df.reindex(common_time_range)

# Fill NaN values with forward fill
ntc_df = ntc_df.fillna(method="ffill")
water_reservoirs = water_reservoirs.fillna(method="ffill")
wind_solar_forecast_df = wind_solar_forecast_df.fillna(method="ffill")

# Merge
merged_df = da_prices.merge(weather_df, left_index=True, right_index=True, how="left")
merged_df = merged_df.merge(load_forecast_df, left_index=True, right_index=True, how="left")
merged_df = merged_df.merge(ntc_df, left_index=True, right_index=True, how="left")
merged_df = merged_df.merge(water_reservoirs, left_index=True, right_index=True, how="left")
merged_df = merged_df.merge(wind_solar_forecast_df, left_index=True, right_index=True, how="left")


# Drop columns that contain only NaN or all zeros
merged_df = merged_df.dropna(axis=1, how='all')  # Drop columns with all NaN values
merged_df = merged_df.loc[:, (merged_df.fillna(0) != 0).any(axis=0)]  # Drop columns with all zeros

merged_df

"""# ProfileReport"""

import pandas as pd
from ydata_profiling import ProfileReport
from IPython.core.display import display, HTML


profile = ProfileReport(merged_df, explorative=True, title="Merged Data Summary")
display(HTML(profile.to_notebook_iframe()))

"""# Raw Data Visualization"""

import matplotlib.pyplot as plt

n_cols = 2
n_rows = len(merged_df.columns) // n_cols + (len(merged_df.columns) % n_cols > 0)

fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, n_rows * 4))
axes = axes.flatten()

for idx, (col, ax) in enumerate(zip(merged_df.columns, axes)):
    merged_df[col].plot(ax=ax, legend=False)
    ax.set_title(col)
    ax.grid(True, linestyle="--", alpha=0.7)

plt.tight_layout()
plt.show()

"""# **Feature Engineering**


*   **Captures important patterns**: Helps the model identify key relationships in the data.

*   **Handles temporal dependencies**: Features like lag and change show how past values affect future ones.

*   **Reveals interactions**: Highlights relationships between variables.

*   **Speeds up learning**: Makes it easier for the model to focus on relevant signals.

*   **Reduces overfitting**: Prevents the model from learning noise.


"""

# Create lagged features
lags = [1, 7, 30]
for lag in lags:
    for col in ["Load_forecast_NO_2", "Generation_forecast_NO_2"]:
        if col in merged_df.columns:
            merged_df[f"{col}_lag_{lag}"] = merged_df[col].shift(lag)

# Add rolling statistics (mean & std deviation)
window_sizes = [3, 7, 30]
for window in window_sizes:
    for col in ["Load_forecast_NO_2", "Generation_forecast_NO_2"]:
        if col in merged_df.columns:
            merged_df[f"{col}_roll_mean_{window}"] = merged_df[col].rolling(window=window).mean()
            merged_df[f"{col}_roll_std_{window}"] = merged_df[col].rolling(window=window).std()

# Add relative change features
for col in ["Load_forecast_NO_2", "Generation_forecast_NO_2"]:
    if col in merged_df.columns:
        merged_df[f"{col}_pct_change"] = merged_df[col].pct_change()

# Add interaction features
if "Load_forecast_NO_2" in merged_df.columns and "Generation_forecast_NO_2" in merged_df.columns:
    merged_df["Load_Generation_ratio"] = (
        merged_df["Load_forecast_NO_2"] / merged_df["Generation_forecast_NO_2"]
    )
    merged_df["Load_Generation_diff"] = (
        merged_df["Load_forecast_NO_2"] - merged_df["Generation_forecast_NO_2"]
    )

# Add datetime features
merged_df["Day_of_Week"] = merged_df.index.dayofweek
merged_df["Month"] = merged_df.index.month
merged_df["Is_Weekend"] = (merged_df["Day_of_Week"] >= 5).astype(int)



# Plot
n_cols = 2
n_rows = len(merged_df.columns) // n_cols + (len(merged_df.columns) % n_cols > 0)

fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, n_rows * 4))
axes = axes.flatten()

for idx, (col, ax) in enumerate(zip(merged_df.columns, axes)):
    merged_df[col].plot(ax=ax, legend=False)
    ax.set_title(col)
    ax.grid(True, linestyle="--", alpha=0.7)

plt.tight_layout()
plt.show()

"""# Build X and Y"""

# Drop columns with NaN or all zeros
merged_df = merged_df.dropna(axis=1, how='all')  # Drop columns with all NaN values
merged_df = merged_df.loc[:, (merged_df.fillna(0) != 0).any(axis=0)]  #Drop columns with all zeros

# Handle missing rows
df_cleaned = merged_df.dropna()

df_cleaned



# Create universal X and Y data for all models

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split


# create sequences for all models
def create_sequences(data, target_column, window_size=30, horizon=1):
    X, y = [], []
    for i in range(len(data) - window_size - horizon + 1):
        X.append(data.iloc[i:(i + window_size)].values)
        y.append(data.iloc[i + window_size:i + window_size + horizon][target_column].values)
    return np.array(X), np.array(y)


# Set target column and window size
target_column = 'DA_prices_NO_2'
window_size = 30
horizon = 1

# Select feature columns
feature_columns = df_cleaned.columns.tolist()


# Create feature dataset
data = df_cleaned[feature_columns].copy()


# Normalize data
scaler = MinMaxScaler()
data_scaled = pd.DataFrame(
    scaler.fit_transform(data),
    columns=data.columns,
    index=data.index
)


# Create separate scaler for prices for inverse transformation
price_scaler = MinMaxScaler()
price_scaler.fit_transform(df_cleaned[[target_column]])


# Create sequences
X, y = create_sequences(data_scaled, target_column, window_size, horizon)


print(f"X shape: {X.shape}")
print(f"y shape: {y.shape}")


# Split into train and test sets
test_size = 0.2
split_idx = int(len(X) * (1 - test_size))

X_train, X_test = X[:split_idx], X[split_idx:]
y_train, y_test = y[:split_idx], y[split_idx:]

print(f"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}")
print(f"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}")


# Save feature names list for later analysis
feature_names = data.columns.tolist()


# Display sample sequences
print("\nSample input sequence (first 3 time steps of first sample):")
print(X_train[0, :3, :])
print("\nCorresponding target value:")
print(y_train[0])

"""# Build TCN model

"""

# Basic TCN Model Implementation

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Conv1D, Dropout, Activation, SpatialDropout1D
from tensorflow.keras.layers import BatchNormalization, LayerNormalization, Add


# Create TCN Residual Block
def residual_block(x, dilation_rate, nb_filters, kernel_size, padding, dropout_rate=0.2):

    prev_x = x
    # Layer Normalization
    x = LayerNormalization()(x)

    # First convolution layer
    x = Conv1D(filters=nb_filters, kernel_size=kernel_size, dilation_rate=dilation_rate,
              padding=padding)(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = SpatialDropout1D(dropout_rate)(x)

    # Second convolution layer
    x = Conv1D(filters=nb_filters, kernel_size=kernel_size, dilation_rate=dilation_rate,
              padding=padding)(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = SpatialDropout1D(dropout_rate)(x)


    # If input and output dimensions are different, add 1x1 convolution to adjust dimensions
    if prev_x.shape[-1] != nb_filters:
        prev_x = Conv1D(nb_filters, 1, padding='same')(prev_x)

    # Add skip connection
    res = Add()([prev_x, x])
    return res


# Create TCN Model
def create_tcn_model(input_shape, output_units=1, nb_filters=64, kernel_size=3,
                   dilations=[1, 2, 4, 8, 16], dropout_rate=0.2):

    inputs = Input(shape=input_shape)
    x = inputs


    # Causal padding to ensure the model doesn't use "future" information
    padding = 'causal'

    # Add TCN layers
    for dilation_rate in dilations:
        x = residual_block(x, dilation_rate, nb_filters, kernel_size, padding, dropout_rate)

    # Add output layer
    x = Dense(output_units)(x)

    # Build model
    model = Model(inputs=inputs, outputs=x)

    return model


# Build and compile TCN baseline model


# Set parameters
input_shape = (X_train.shape[1], X_train.shape[2])  # (time_steps, features)
output_units = 1  # Predict one value: electricity price
nb_filters = 64  # Number of convolution filters
kernel_size = 3  #  Kernel size
dilations = [1, 2, 4, 8, 16]  #  List of dilation rates
dropout_rate = 0.2  # Dropout rate


# Create baseline TCN model
tcn_model = create_tcn_model(
    input_shape=input_shape,
    output_units=output_units,
    nb_filters=nb_filters,
    kernel_size=kernel_size,
    dilations=dilations,
    dropout_rate=dropout_rate
)


# Compile model
tcn_model.compile(
    optimizer='adam',
    loss='mean_squared_error'
)


# Print model summary
tcn_model.summary()

# Train model
history = tcn_model.fit(
    X_train, y_train,
    epochs=30,
    batch_size=32,
    validation_data=(X_test, y_test),
    verbose=1
)

# Plot training history
plt.figure(figsize=(10, 6))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('TCN Baseline Model: Training vs Validation Loss')
plt.ylabel('Loss')
plt.xlabel('Epochs')
plt.legend()
plt.show()



# Evaluate model
test_loss = tcn_model.evaluate(X_test, y_test, verbose=0)
print(f"Test loss (MSE): {test_loss}")

# Make predictions
y_pred = tcn_model.predict(X_test)


# Inverse transform predictions and actual values (if needed)
def inverse_transform(values):

    reshaped_values = values.reshape(-1, 1)
    original_values = price_scaler.inverse_transform(reshaped_values)
    return original_values


# Inverse transform
y_test_orig = inverse_transform(y_test).flatten()
y_pred_orig = inverse_transform(y_pred).flatten()


# Plot predictions
N = 2000
plt.figure(figsize=(15, 6))
plt.plot(y_test_orig[:N], label='Actual Values', color='blue')
plt.plot(y_pred_orig[:N], label='Predicted Values', color='red', alpha=0.7)
plt.title('TCN Baseline Model: Actual vs Predicted Day-Ahead Prices')
plt.ylabel('Price (EUR/MWh)')
plt.xlabel('Samples')
plt.legend()
plt.show()

# TCN Model Hyperparameter Tuning

import tensorflow as tf
from tensorflow.keras.callbacks import EarlyStopping
import matplotlib.pyplot as plt
import numpy as np


# Add early stopping callback
early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True
)


# Define function to create TCN model with more hyperparameter options
def create_tcn_model_with_params(
    input_shape,
    output_units=1,
    nb_filters=64,
    kernel_size=3,
    dilations=[1, 2, 4, 8, 16],
    dropout_rate=0.2,
    learning_rate=0.001
):

    inputs = Input(shape=input_shape)
    x = inputs
    padding = 'causal'

    # Add TCN layers
    for dilation_rate in dilations:
        x = residual_block(x, dilation_rate, nb_filters, kernel_size, padding, dropout_rate)

    # Add output layer
    x = Dense(output_units)(x)

    # Build model
    model = Model(inputs=inputs, outputs=x)

    # Compile model
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
        loss='mean_squared_error'
    )

    return model


# Set learning rates and batch sizes to try
learning_rates = [0.01, 0.001, 0.0001]
batch_sizes = [32, 64, 128]


# Dictionary to store results
results = {}

# Fine-tune learning rate and batch size
for lr in learning_rates:
    for bs in batch_sizes:
        print(f"Training TCN model with learning_rate={lr}, batch_size={bs}")


        # Create model
        model = create_tcn_model_with_params(
            input_shape=(X_train.shape[1], X_train.shape[2]),
            output_units=1,
            nb_filters=64,
            kernel_size=3,
            dilations=[1, 2, 4, 8, 16],
            dropout_rate=0.2,
            learning_rate=lr
        )


        # Train model
        history = model.fit(
            X_train, y_train,
            epochs=30,
            batch_size=bs,
            validation_data=(X_test, y_test),
            callbacks=[early_stopping],
            verbose=1
        )


        # Evaluate model
        test_loss = model.evaluate(X_test, y_test, verbose=0)


        # Store results
        key = f"lr_{lr}_bs_{bs}"
        results[key] = {
            'model': model,
            'history': history,
            'test_loss': test_loss,
            'epochs_trained': len(history.history['loss'])
        }

        print(f"Test loss (MSE): {test_loss}")


        # Plot training history
        plt.figure(figsize=(10, 6))
        plt.plot(history.history['loss'], label='Training Loss')
        plt.plot(history.history['val_loss'], label='Validation Loss')
        plt.title(f'TCN Model: lr={lr}, bs={bs}')
        plt.ylabel('Loss')
        plt.xlabel('Epochs')
        plt.legend()
        plt.show()

# Find the best performing model
best_model_key = min(results, key=lambda k: results[k]['test_loss'])
best_model = results[best_model_key]['model']
best_lr = float(best_model_key.split('_')[1])
best_bs = int(best_model_key.split('_')[3])

print(f"Best model hyperparameters:")
print(f"Learning Rate: {best_lr}")
print(f"Batch Size: {best_bs}")
print(f"Test Loss (MSE): {results[best_model_key]['test_loss']}")


# Make predictions and visualize for the best model
y_pred_best = best_model.predict(X_test)


# Inverse transform
y_test_orig = inverse_transform(y_test).flatten()
y_pred_best_orig = inverse_transform(y_pred_best).flatten()


# Plot predictions
N = 2000
plt.figure(figsize=(15, 6))
plt.plot(y_test_orig[:N], label='Actual Values', color='blue')
plt.plot(y_pred_best_orig[:N], label='Predicted Values', color='red', alpha=0.7)
plt.title('Best TCN Model: Actual vs Predicted Day-Ahead Prices')
plt.ylabel('Price (EUR/MWh)')
plt.xlabel('Samples')
plt.legend()
plt.show()


# Output best parameters for use in the next step
print("Best parameters to use in the next step:")
print(f"learning_rate = {best_lr}")
print(f"batch_size = {best_bs}")

# Optimized TCN Model Implementation

import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Conv1D, Dropout, Activation, SpatialDropout1D
from tensorflow.keras.layers import BatchNormalization, LayerNormalization, Add, GlobalAveragePooling1D
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from tensorflow.keras.regularizers import l2
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Use the best learning rate and batch size found in the previous step

best_lr = 0.01
best_bs = 64


# Optimized residual block with regularization and additional normalization
def optimized_residual_block(x, dilation_rate, nb_filters, kernel_size, padding,
                           dropout_rate=0.2, l2_reg=0.0001):

    prev_x = x

    # Layer Normalization
    x = LayerNormalization()(x)

    # First convolution layer
    x = Conv1D(filters=nb_filters, kernel_size=kernel_size, dilation_rate=dilation_rate,
              padding=padding, kernel_regularizer=l2(l2_reg))(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = SpatialDropout1D(dropout_rate)(x)

    # Second convolution layer
    x = Conv1D(filters=nb_filters, kernel_size=kernel_size, dilation_rate=dilation_rate,
              padding=padding, kernel_regularizer=l2(l2_reg))(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = SpatialDropout1D(dropout_rate)(x)


    # If input and output dimensions are different, add 1x1 convolution to adjust dimensions
    if prev_x.shape[-1] != nb_filters:
        prev_x = Conv1D(nb_filters, 1, padding='same', kernel_regularizer=l2(l2_reg))(prev_x)

    # Add skip connection
    res = Add()([prev_x, x])
    return res


# Create optimized TCN model
def create_optimized_tcn_model(
    input_shape,
    output_units=1,
    nb_filters=64,
    kernel_size=3,
    dilations=[1, 2, 4, 8, 16, 32], # Increase dilation rates to expand receptive field
    dropout_rate=0.3,               # Increase dropout to reduce overfitting
    l2_reg=0.0001,                  # Add L2 regularization
    learning_rate=0.001
):

    inputs = Input(shape=input_shape)
    x = inputs
    padding = 'causal'


    # Extract skip connections for multi-scale feature fusion
    skip_connections = []

    # Add TCN layers
    for dilation_rate in dilations:
        x = optimized_residual_block(
            x, dilation_rate, nb_filters, kernel_size, padding, dropout_rate, l2_reg
        )
        skip_connections.append(x)


    # Fuse multi-scale features (similar to U-Net skip connections)
    if len(skip_connections) > 1:
        x = Add()(skip_connections)


    # Global average pooling to reduce overfitting
    x = GlobalAveragePooling1D()(x)

    # Add output layer
    x = Dense(output_units, kernel_regularizer=l2(l2_reg))(x)

    # Build model
    model = Model(inputs=inputs, outputs=x)

    # Compile model
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
        loss='mean_squared_error'
    )

    return model


# Set up callbacks
early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=10,
    restore_best_weights=True,
    verbose=1
)

reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=5,
    min_lr=1e-6,
    verbose=1
)


# Create optimized TCN model
optimized_tcn_model = create_optimized_tcn_model(
    input_shape=(X_train.shape[1], X_train.shape[2]),
    output_units=1,
    nb_filters=128,                        # Increase filters for higher capacity
    kernel_size=5,                         # Increase kernel size to capture more context
    dilations=[1, 2, 4, 8, 16, 32, 64],    # Increase dilation rates for larger receptive field
    dropout_rate=0.3,                      # Adjust dropout to reduce overfitting
    l2_reg=0.0001,                         # Add L2 regularization
    learning_rate=best_lr                  # Use the best learning rate found
)


# Print model summary
optimized_tcn_model.summary()

# Train model
history = optimized_tcn_model.fit(
    X_train, y_train,
    epochs=30,
    batch_size=best_bs,  # Use the best batch size found
    validation_data=(X_test, y_test),
    callbacks=[early_stopping, reduce_lr],
    verbose=1
)

# Plot training history
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('TCN Advanced Model: Training vs Validation Loss')
plt.ylabel('Loss')
plt.xlabel('Epochs')
plt.legend()

if 'lr' in history.history:
    plt.subplot(1, 2, 2)
    plt.plot(history.history['lr'])
    plt.title('Learning Rate Decay')
    plt.ylabel('Learning Rate')
    plt.xlabel('Epochs')

plt.tight_layout()
plt.show()


# Evaluate model
test_loss = optimized_tcn_model.evaluate(X_test, y_test, verbose=0)
print(f"Test loss (MSE): {test_loss}")

# Make predictions
y_pred = optimized_tcn_model.predict(X_test)


# Inverse transform predictions and actual values
y_test_orig = inverse_transform(y_test).flatten()
y_pred_orig = inverse_transform(y_pred).flatten()


# Plot predictions
N = 2000
plt.figure(figsize=(15, 6))
plt.plot(y_test_orig[:N], label='Actual Values', color='blue')
plt.plot(y_pred_best_orig[:N], label='Predicted Values', color='red', alpha=0.7)
plt.title('Best TCN Model: Actual vs Predicted Day-Ahead Prices')
plt.ylabel('Price (EUR/MWh)')
plt.xlabel('Samples')
plt.legend()
plt.show()


# Calculate multiple evaluation metrics
mse = mean_squared_error(y_test_orig, y_pred_orig)
rmse = np.sqrt(mse)
mae = mean_absolute_error(y_test_orig, y_pred_orig)
r2 = r2_score(y_test_orig, y_pred_orig)

print(f"MSE: {mse}")
print(f"RMSE: {rmse}")
print(f"MAE: {mae}")
print(f"R2 Score: {r2}")
