{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R_0UjNuEthf8"
   },
   "source": [
    "# **Introduction**\n",
    "\n",
    "This project focuses on predicting day-ahead electricity prices for the NO2 zone in Norway. The analysis incorporates various energy-related data, such as forecasts for load, generation, wind, and solar energy, as well as cross-border physical flows and net transfer capacities. The goal is to build a comprehensive dataset that can be used for predictive modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9gTjUMztGSS"
   },
   "source": [
    "# **Data Collection**\n",
    "Data is collected using the **ENTSO-E API**.\n",
    "\n",
    "The following datasets are retrieved:\n",
    "\n",
    "1.   **Day-Ahead Prices**: Historical day-ahead electricity prices for the NO2 zone.\n",
    "2.   **Load Forecasts**: Forecasted electricity demand for multiple areas, including NO2 and neighboring regions.\n",
    "3.   **Wind and Solar Forecasts**: Forecasted renewable energy production (wind and solar) for relevant zones.\n",
    "1.   **Generation Forecasts**: Predicted electricity generation capacity for selected regions.\n",
    "1.   **Net Transfer Capacities (NTC)**: Week-ahead net transfer capacities for specific cross-border connections.\n",
    "1.   **Cross-Border Physical Flows**: Net flows of electricity across borders involving the NO2 zone.\n",
    "\n",
    "The analysis spans from October 2023 to September 2024. To account for lagged features, the dataset includes an extended start date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5JShAkgQSBgh",
    "outputId": "69b6f974-bcb5-4b8f-8f4f-7f5db8c34c4b"
   },
   "outputs": [],
   "source": [
    "#!{sys.executable} -m pip install entsoe-py\n",
    "!pip install entsoe-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oRz7gyVqW6FH",
    "outputId": "8635d9eb-9fd6-4e38-fb67-f7eff1e022b7"
   },
   "outputs": [],
   "source": [
    "# 1. Imports and API Data Collection\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from entsoe import EntsoePandasClient\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy import stats\n",
    "\n",
    "# Initialize API Client\n",
    "client = EntsoePandasClient(api_key='d43c0033-144a-4c29-aa12-98d6d1070332')\n",
    "\n",
    "# Define date range\n",
    "start = pd.Timestamp('20231001', tz='Europe/Brussels')\n",
    "end = pd.Timestamp('20240930', tz='Europe/Brussels')\n",
    "extended_start = start - pd.Timedelta(days=1)\n",
    "\n",
    "# Define specific areas and variable mappings\n",
    "areas_load_forecast = [\"NO_2\", \"NO_1\", \"NO_5\", \"DK\", \"NL\", \"DE_LU\"]\n",
    "areas_wind_and_solar_forecast = areas_load_forecast  # Same as load forecast areas\n",
    "areas_generation_forecast = areas_load_forecast  # Same as load forecast areas\n",
    "ntc_pairs = [\n",
    "    (\"NO_2\", \"NL\"),\n",
    "    (\"NO_2\", \"DK1\"),\n",
    "    (\"NO_2\", \"DE_LU\"),\n",
    "    (\"NO_2\", \"GB\"),\n",
    "    (\"NO_2\", \"NO_5\")\n",
    "]  # Net transfer capacity\n",
    "\n",
    "# Cross-Border Physical Flows for NO2 Zone\n",
    "crossborder_pairs = ntc_pairs  # Use the existing NTC pairs for cross-border flow calculations\n",
    "\n",
    "data_frames = {}\n",
    "\n",
    "# Fetch Day-Ahead Prices for NO_2\n",
    "try:\n",
    "    data_frames['DA_prices_NO_2'] = client.query_day_ahead_prices(\"NO_2\", start=extended_start, end=end).to_frame(name='DA_prices_NO_2')\n",
    "    # Add Previous Day's Price as a Feature\n",
    "    data_frames['DA_prices_NO_2']['Prev_Day_DA_prices_NO_2'] = data_frames['DA_prices_NO_2']['DA_prices_NO_2'].shift(1)\n",
    "    # Filter to remove the extended day\n",
    "    data_frames['DA_prices_NO_2'] = data_frames['DA_prices_NO_2'].loc[start:end]\n",
    "except Exception as e:\n",
    "    print(f\"Failed to fetch DA prices for NO_2: {e}\")\n",
    "\n",
    "# Fetch Load Forecast for multiple areas\n",
    "for area in areas_load_forecast:\n",
    "    try:\n",
    "        data = client.query_load_forecast(area, start=start, end=end).rename(columns={'Forecasted Load': f'Load_forecast_{area}'})\n",
    "        data_frames[f'Load_forecast_{area}'] = data\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch Load Forecast for {area}: {e}\")\n",
    "\n",
    "# Fetch Wind and Solar Forecast for multiple areas\n",
    "for area in areas_wind_and_solar_forecast:\n",
    "    try:\n",
    "        data = client.query_wind_and_solar_forecast(area, start=start, end=end)\n",
    "        data.columns = [f\"{col}_{area}\" for col in data.columns]  # Consistent column naming\n",
    "        data_frames[f'Wind_and_Solar_forecast_{area}'] = data\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch Wind and Solar Forecast for {area}: {e}\")\n",
    "\n",
    "# Fetch Generation Forecast for multiple areas\n",
    "for area in areas_generation_forecast:\n",
    "    try:\n",
    "        data = client.query_generation_forecast(area, start=start, end=end).to_frame(name=f'Generation_forecast_{area}')\n",
    "        data_frames[f'Generation_forecast_{area}'] = data\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch Generation Forecast for {area}: {e}\")\n",
    "\n",
    "# Fetch Net Transfer Capacity (Week-Ahead) for specified pairs\n",
    "for from_area, to_area in ntc_pairs:\n",
    "    try:\n",
    "        data = client.query_net_transfer_capacity_weekahead(from_area, to_area, start=start, end=end).to_frame(\n",
    "            name=f'NTC_WeekAhead_{from_area}_to_{to_area}')\n",
    "        data_frames[f'NTC_WeekAhead_{from_area}_to_{to_area}'] = data\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch NTC Week-Ahead from {from_area} to {to_area}: {e}\")\n",
    "\n",
    "# Fetch Aggregate Water Reservoirs and Hydro Storage for NO_2\n",
    "try:\n",
    "    data_frames['Aggregate_Water_Reservoirs_NO_2'] = client.query_aggregate_water_reservoirs_and_hydro_storage(\n",
    "        \"NO_2\", start=start, end=end).to_frame(name='Aggregate_Water_Reservoirs_NO_2')\n",
    "except Exception as e:\n",
    "    print(f\"Failed to fetch Aggregate Water Reservoirs for NO_2: {e}\")\n",
    "\n",
    "# Fetch and compute net flows for each pair\n",
    "def fetch_net_flow(from_area, to_area):\n",
    "    try:\n",
    "        flow_1 = client.query_crossborder_flows(from_area, to_area, start=start, end=end)\n",
    "        flow_2 = client.query_crossborder_flows(to_area, from_area, start=start, end=end)\n",
    "        net_flow = flow_1 - flow_2\n",
    "        net_flow.name = f\"Net_Flow_{from_area}_to_{to_area}\"\n",
    "        return net_flow.to_frame()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch cross-border flows between {from_area} and {to_area}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "for from_area, to_area in crossborder_pairs:\n",
    "    data_frames[f\"Net_Flow_{from_area}_to_{to_area}\"] = fetch_net_flow(from_area, to_area)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V6aXXYOMu3dL"
   },
   "source": [
    "All datasets are merged into a single DataFrame for analysis, ensuring all datasets align on the same timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x2Q6xgw0pFTf",
    "outputId": "826e27d2-e8cc-4f97-b7a0-39920951992d"
   },
   "outputs": [],
   "source": [
    "merged_data = pd.concat(data_frames.values(), axis=1)\n",
    "\n",
    "merged_data.info()\n",
    "print(merged_data.describe())\n",
    "print(merged_data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data['Net_Flow_NO_2_to_GB']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xnqw1lISupbE"
   },
   "source": [
    "Missing values are addressed using forward-fill and backward-fill techniques where appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F1VMWWH452PE",
    "outputId": "fa8b0cfa-5390-4e50-ac49-0da60f38b7b8"
   },
   "outputs": [],
   "source": [
    "# Filter rows where 'DA_prices_NO_2' is not NaN\n",
    "merged_data = merged_data.dropna(subset=['DA_prices_NO_2'])\n",
    "\n",
    "# Check the result\n",
    "print(merged_data.info())\n",
    "\n",
    "\n",
    "merged_data = merged_data.ffill()\n",
    "merged_data = merged_data.bfill()\n",
    "print(\"After fill the data\")\n",
    "print(merged_data.info())\n",
    "\n",
    "print(merged_data.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wO457I9uxbYA"
   },
   "source": [
    "# **Exploratory Data Analysis (EDA)**\n",
    "\n",
    "EDA is conducted to understand the distributions, relationships, and variability of the collected data:\n",
    "- **Target Variable**: The distribution of day-ahead prices (`DA_prices_NO_2`) is analyzed for trends and seasonality.\n",
    "- **Feature Distributions**: Boxplots and histograms reveal variability and outliers in key features.\n",
    "- **Correlation Analysis**: A heatmap highlights relationships between features and the target variable.\n",
    "- **Temporal Patterns**: Trends in electricity prices and other features over time are visualized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qGK2_IWbvK1p",
    "outputId": "5effe6ce-b08e-418d-cc7d-c00754be9636",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot time-series of DA_prices_NO_2\n",
    "merged_data[\"DA_prices_NO_2\"].plot(figsize=(12, 6))\n",
    "plt.title(\"Time Series of DA_prices_NO_2\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Price (EUR/MWh)\")\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Visualize the distribution of the target variable\n",
    "sns.histplot(merged_data[\"DA_prices_NO_2\"], kde=True, color='blue')\n",
    "plt.title('Distribution of DA_prices_NO_2')\n",
    "plt.xlabel('Price (EUR/MWh)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Calculate mean and standard deviation of the target variable\n",
    "mean_price = merged_data[\"DA_prices_NO_2\"].mean()\n",
    "std_price = merged_data[\"DA_prices_NO_2\"].std()\n",
    "print(f\"Mean Price: {mean_price:.2f} EUR/MWh, Standard Deviation: {std_price:.2f} EUR/MWh\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ufmYwwXJXV55",
    "outputId": "4c48a4ec-1b7e-4c94-99b0-3d9269b78906",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize the features\n",
    "\n",
    "# Determine the number of columns in merged_data\n",
    "num_plots = len(merged_data.columns)\n",
    "\n",
    "# Dynamically calculate rows and columns\n",
    "num_cols = 4\n",
    "num_rows = (num_plots + num_cols - 1) // num_cols\n",
    "\n",
    "# Create the figure with adjusted size\n",
    "plt.figure(figsize=(15, num_rows * 5))\n",
    "\n",
    "# Plot a box plot for each column in merged_data\n",
    "for i, column in enumerate(merged_data.columns, start=1):\n",
    "    plt.subplot(num_rows, num_cols, i)\n",
    "    sns.boxplot(y=merged_data[column], color=np.random.rand(3,))\n",
    "    plt.title(f'Box Plot of {column}', fontsize=10)\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')\n",
    "\n",
    "plt.tight_layout()  # Prevent overlap\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pPE4qSgOJp4r",
    "outputId": "4df6a084-fce8-4ac9-e0d0-1e1a91b1dfb5"
   },
   "outputs": [],
   "source": [
    "# Identify features with all zero values\n",
    "all_zero_features = [col for col in merged_data.columns if (merged_data[col] == 0).all()]\n",
    "\n",
    "# Identify features with 25%, 50%, and 75% values being the same (low variability)\n",
    "low_variability_features = [\n",
    "    col for col in merged_data.columns\n",
    "    if merged_data[col].describe()[['25%', '50%', '75%']].nunique() == 1\n",
    "]\n",
    "\n",
    "# Combine both lists of features to drop\n",
    "features_to_drop = set(all_zero_features + low_variability_features)\n",
    "\n",
    "# Drop these features from the DataFrame\n",
    "merged_data = merged_data.drop(columns=features_to_drop)\n",
    "\n",
    "print(f\"Removed features: {features_to_drop}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FvKMHa704w5Y",
    "outputId": "c163fa2f-333f-460f-c0ee-c660d77ee35c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(merged_data.isna().sum())\n",
    "merged_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zbrVPPRxXtQD",
    "outputId": "f9123bd1-c3a8-41ee-d675-244bf3b8b2f5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Outlier Detection\n",
    "# Calculate IQR for each numeric column\n",
    "Q1 = merged_data.quantile(0.25)  # First quartile (25th percentile)\n",
    "Q3 = merged_data.quantile(0.75)  # Third quartile (75th percentile)\n",
    "IQR = Q3 - Q1  # Interquartile range\n",
    "\n",
    "# Define the outlier threshold\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Filter out rows where any column's value is an outlier\n",
    "merged_data_no_outliers = merged_data[~((merged_data < lower_bound) | (merged_data > upper_bound)).any(axis=1)]\n",
    "\n",
    "# Data Visualization: Box Plots After Removing Outliers\n",
    "\n",
    "plt.figure(figsize=(15, num_rows * 5))\n",
    "\n",
    "# Plot a box plot for each column in merged_data_no_outliers (after removing outliers)\n",
    "for i, column in enumerate(merged_data_no_outliers.columns, start=1):\n",
    "    plt.subplot(num_rows, num_cols, i)\n",
    "    sns.boxplot(y=merged_data_no_outliers[column], color=np.random.rand(3,))\n",
    "    plt.title(f'Box Plot of {column}', fontsize=10)\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j3kWDNFODF3q",
    "outputId": "4ce97bd9-b06e-4d96-bcae-762aa89f457d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"After removing outliers:\")\n",
    "print(merged_data_no_outliers.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "eriDQnMjvfNc",
    "outputId": "69747209-9496-4cc1-8237-cfb04f082aed",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add a 'Weekday' column (0=Monday, 6=Sunday) and a 'Weekend' column (1=Weekend, 0=Weekday)\n",
    "merged_data_no_outliers['Weekday'] = merged_data_no_outliers.index.dayofweek\n",
    "merged_data_no_outliers['Is_Weekend'] = (merged_data_no_outliers['Weekday'] >= 5).astype(int)\n",
    "\n",
    "# Load forecasts on weekdays vs weekends\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.boxplot(data=merged_data_no_outliers, x='Is_Weekend', y='Load_forecast_NO_2', palette='coolwarm')\n",
    "plt.xticks([0, 1], ['Weekday', 'Weekend'])\n",
    "plt.title('Load Forecast Variability: Weekday vs Weekend')\n",
    "plt.xlabel('Day Type')\n",
    "plt.ylabel('Load Forecast (MW)')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "# NTC values against day-ahead prices\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.scatterplot(data=merged_data_no_outliers, x='NTC_WeekAhead_NO_2_to_NL', y='DA_prices_NO_2', alpha=0.7)\n",
    "plt.title('NTC vs Day-Ahead Prices')\n",
    "plt.xlabel('Net Transfer Capacity (MW)')\n",
    "plt.ylabel('Day-Ahead Prices (EUR/MWh)')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "# Net flows against day-ahead prices\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.scatterplot(data=merged_data_no_outliers, x='Net_Flow_NO_2_to_GB', y='DA_prices_NO_2', alpha=0.7)\n",
    "plt.title('Net Flow vs Day-Ahead Prices')\n",
    "plt.xlabel('Net Flow (MW)')\n",
    "plt.ylabel('Day-Ahead Prices (EUR/MWh)')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_data_no_outliers.Weekday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LA1KckWIXY_k",
    "outputId": "4c131462-8b87-47d6-ad9c-bbcba9e7e368"
   },
   "outputs": [],
   "source": [
    "# Correlation Matrix Heatmap\n",
    "\n",
    "threshold = 0.1\n",
    "\n",
    "correlation_matrix = merged_data_no_outliers.corr()\n",
    "\n",
    "# Filter out features with low correlation to the target or low maximum correlation overall\n",
    "low_corr_features = [\n",
    "    col for col in correlation_matrix.columns\n",
    "    if (abs(correlation_matrix.loc['DA_prices_NO_2', col]) < threshold) or\n",
    "       (correlation_matrix[col].abs().max() < threshold)\n",
    "]\n",
    "\n",
    "# Filter the correlation matrix to exclude low-correlation features\n",
    "filtered_corr_matrix = correlation_matrix.drop(index=low_corr_features, columns=low_corr_features)\n",
    "\n",
    "# Mask the upper triangle of the filtered correlation matrix\n",
    "mask = np.triu(np.ones_like(filtered_corr_matrix, dtype=bool))\n",
    "\n",
    "# Create the heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(filtered_corr_matrix, annot=True, cmap='coolwarm', cbar=True,\n",
    "            mask=mask, annot_kws={\"size\": 8}, fmt='.2f', square=True)\n",
    "\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0, ha='right')\n",
    "plt.title('Filtered Correlation Matrix (Triangle Format)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "vOnBHLG11pGC",
    "outputId": "d7493aa3-d5a0-4552-99f4-059c90ce89f3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate pair plots for the entire DataFrame\n",
    "sns.pairplot(merged_data_no_outliers)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h_ZvUljazK17",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Determine the number of columns\n",
    "number_cols = merged_data_no_outliers.shape[1]\n",
    "\n",
    "slice_size = 4\n",
    "\n",
    "# Iterate over the dataset, slicing it into chunks of 4 columns\n",
    "for start_col in range(0, number_cols, slice_size):\n",
    "\n",
    "    end_col = min(start_col + slice_size, number_cols)\n",
    "    data_slice = merged_data_no_outliers.iloc[:, start_col:end_col]\n",
    "\n",
    "\n",
    "    print(f\"Visualizing columns {start_col + 1} to {end_col}: {list(data_slice.columns)}\")\n",
    "\n",
    "\n",
    "    sns.pairplot(data_slice,plot_kws={'s': 10})\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8BNRFSIH3J7R"
   },
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix\n",
    "correlation_matrix = merged_data_no_outliers.corr()\n",
    "\n",
    "# Display correlations of all variables with the target variable\n",
    "relevant_features = correlation_matrix['DA_prices_NO_2'].sort_values(ascending=False)\n",
    "print(\"Correlation with DA_prices_NO2:\\n\", relevant_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "emRkro0o5aUu",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "\n",
    "# Add a constant column to the data for VIF calculation (the intercept in a regression model)\n",
    "X = add_constant(merged_data_no_outliers.drop('DA_prices_NO_2', axis=1))\n",
    "\n",
    "# Calculate VIF for each feature\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['Feature'] = X.columns\n",
    "vif_data['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "# Display VIF for each feature\n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-7ELZ-z5zA5i"
   },
   "source": [
    "# **Feature Engineering**\n",
    "\n",
    "To enhance the predictive power of the dataset, the following features are engineered:\n",
    "1. **Lagged Features**: Historical values for load and generation forecasts at intervals of 1, 7, and 30 days.\n",
    "2. **Rolling Statistics**: Rolling means and standard deviations for load and generation forecasts over 3, 7, and 30 days.\n",
    "3. **Relative Changes**: Percentage changes in load and generation forecasts.\n",
    "4. **Interaction Terms**: Ratios and differences between load and generation values.\n",
    "5. **Temporal Features**: Day of the week, month, and weekend indicators are added to capture seasonality and periodic effects.\n",
    "\n",
    "These features are critical for capturing patterns in electricity demand, supply, and market dynamics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VwYAfqTaX2wv",
    "outputId": "d0125561-42d0-4350-b2ae-ab0f72f34664"
   },
   "outputs": [],
   "source": [
    "merged_data_no_outliers = merged_data_no_outliers.copy()\n",
    "\n",
    "# Create lagged features for load and generation forecasts\n",
    "lags = [1, 7, 30]\n",
    "for lag in lags:\n",
    "    merged_data_no_outliers.loc[:, f'Load_forecast_NO_2_lag_{lag}'] = merged_data_no_outliers['Load_forecast_NO_2'].shift(lag)\n",
    "    merged_data_no_outliers.loc[:, f'Generation_forecast_NO_2_lag_{lag}'] = merged_data_no_outliers['Generation_forecast_NO_2'].shift(lag)\n",
    "\n",
    "# Add rolling statistics features (mean and SD)\n",
    "window_sizes = [3, 7, 30]\n",
    "for window in window_sizes:\n",
    "    merged_data_no_outliers.loc[:, f'Load_forecast_NO_2_roll_mean_{window}'] = (\n",
    "        merged_data_no_outliers['Load_forecast_NO_2'].rolling(window=window).mean()\n",
    "    )\n",
    "    merged_data_no_outliers.loc[:, f'Load_forecast_NO_2_roll_std_{window}'] = (\n",
    "        merged_data_no_outliers['Load_forecast_NO_2'].rolling(window=window).std()\n",
    "    )\n",
    "    merged_data_no_outliers.loc[:, f'Generation_forecast_NO_2_roll_mean_{window}'] = (\n",
    "        merged_data_no_outliers['Generation_forecast_NO_2'].rolling(window=window).mean()\n",
    "    )\n",
    "    merged_data_no_outliers.loc[:, f'Generation_forecast_NO_2_roll_std_{window}'] = (\n",
    "        merged_data_no_outliers['Generation_forecast_NO_2'].rolling(window=window).std()\n",
    "    )\n",
    "\n",
    "# Add relative change features\n",
    "merged_data_no_outliers.loc[:, 'Load_forecast_NO_2_pct_change'] = (\n",
    "    merged_data_no_outliers['Load_forecast_NO_2'].pct_change()\n",
    ")\n",
    "merged_data_no_outliers.loc[:, 'Generation_forecast_NO_2_pct_change'] = (\n",
    "    merged_data_no_outliers['Generation_forecast_NO_2'].pct_change()\n",
    ")\n",
    "\n",
    "# Add interaction features\n",
    "merged_data_no_outliers.loc[:, 'Load_Generation_ratio'] = (\n",
    "    merged_data_no_outliers['Load_forecast_NO_2'] / merged_data_no_outliers['Generation_forecast_NO_2']\n",
    ")\n",
    "merged_data_no_outliers.loc[:, 'Load_Generation_diff'] = (\n",
    "    merged_data_no_outliers['Load_forecast_NO_2'] - merged_data_no_outliers['Generation_forecast_NO_2']\n",
    ")\n",
    "\n",
    "# Add datetime features\n",
    "merged_data_no_outliers.loc[:, 'Day_of_Week'] = merged_data_no_outliers.index.dayofweek  # 0=Monday, 6=Sunday\n",
    "merged_data_no_outliers.loc[:, 'Month'] = merged_data_no_outliers.index.month\n",
    "merged_data_no_outliers.loc[:, 'Is_Weekend'] = (merged_data_no_outliers['Day_of_Week'] >= 5).astype(int)\n",
    "\n",
    "# Drop rows with NaN values caused by lagging, rolling, and pct_change\n",
    "merged_data_no_outliers.dropna(inplace=True)\n",
    "\n",
    "\n",
    "print(merged_data_no_outliers.head())\n",
    "print(merged_data_no_outliers.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(merged_data_no_outliers.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Matrix Heatmap\n",
    "\n",
    "threshold = 0.4\n",
    "\n",
    "correlation_matrix = merged_data_no_outliers.corr()\n",
    "\n",
    "# Filter out features with low correlation to the target or low maximum correlation overall\n",
    "low_corr_features = [\n",
    "    col for col in correlation_matrix.columns\n",
    "    if (abs(correlation_matrix.loc['DA_prices_NO_2', col]) < threshold) or\n",
    "       (correlation_matrix[col].abs().max() < threshold)\n",
    "]\n",
    "\n",
    "# Filter the correlation matrix to exclude low-correlation features\n",
    "filtered_corr_matrix = correlation_matrix.drop(index=low_corr_features, columns=low_corr_features)\n",
    "\n",
    "# Mask the upper triangle of the filtered correlation matrix\n",
    "mask = np.triu(np.ones_like(filtered_corr_matrix, dtype=bool))\n",
    "\n",
    "# Create the heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(filtered_corr_matrix, annot=True, cmap='coolwarm', cbar=True,\n",
    "            mask=mask, annot_kws={\"size\": 8}, fmt='.2f', square=True)\n",
    "\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0, ha='right')\n",
    "plt.title('Filtered Correlation Matrix (Triangle Format)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix\n",
    "correlation_matrix_2 = merged_data_no_outliers.corr()\n",
    "\n",
    "# Display correlations of all variables with the target variable\n",
    "relevant_features_2 = correlation_matrix_2['DA_prices_NO_2'].sort_values(ascending=False)\n",
    "print(\"Correlation with DA_prices_NO2:\\n\", relevant_features_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bMwYxf-bQSky",
    "outputId": "898f8a92-814e-43c2-ab2c-07f61e6fcd9f"
   },
   "outputs": [],
   "source": [
    "# Ensure a proper DatetimeIndex\n",
    "if not isinstance(merged_data_no_outliers.index, pd.DatetimeIndex):\n",
    "    raise ValueError(\"The index of merged_data_no_outliers must be a DatetimeIndex for proper slicing.\")\n",
    "\n",
    "#Define X,y\n",
    "X = merged_data_no_outliers.drop('DA_prices_NO_2', axis=1)\n",
    "y = merged_data_no_outliers['DA_prices_NO_2']\n",
    "\n",
    "# Train/Test Split\n",
    "train_data = merged_data_no_outliers.loc['2023-10-01':'2024-06-30']\n",
    "test_data = merged_data_no_outliers.loc['2024-07-01':'2024-09-30']\n",
    "\n",
    "# Separate Predictors and Target\n",
    "X_train, y_train = train_data.drop('DA_prices_NO_2', axis=1), train_data['DA_prices_NO_2']\n",
    "X_test, y_test = test_data.drop('DA_prices_NO_2', axis=1), test_data['DA_prices_NO_2']\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"Y_train shape: {y_train.shape}\")\n",
    "print(f\"Y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D-9KXH89S0VZ",
    "outputId": "875d33ce-ac87-4b44-899b-c8a832baefee"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform both training and testing sets\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Verify the scaling by checking mean and standard deviation of the training set\n",
    "scaled_summary = {\n",
    "    \"Training Set Mean (Scaled)\": X_train_scaled.mean(axis=0),\n",
    "    \"Training Set Std Dev (Scaled)\": X_train_scaled.std(axis=0)\n",
    "}\n",
    "\n",
    "scaled_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dGj28G97Ti0x",
    "outputId": "32a533e9-baa4-426b-98b6-694ccaf623d1"
   },
   "outputs": [],
   "source": [
    "# Build the OLS model\n",
    "import statsmodels.api as sm\n",
    "X_train_with_constant = sm.add_constant(X_train_scaled)\n",
    "X_test_with_constant = sm.add_constant(X_test_scaled)\n",
    "\n",
    "# Fit in OLS model\n",
    "ols_model = sm.OLS(y_train, X_train_with_constant).fit()\n",
    "\n",
    "print(ols_model.summary())\n",
    "\n",
    "# Prediction\n",
    "y_pred_ols = ols_model.predict(X_test_with_constant)\n",
    "\n",
    "# Evaluate\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse_ols = mean_squared_error(y_test, y_pred_ols)\n",
    "print(f'Mean Squared Error (OLS): {mse_ols}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "IW3eoDyeOiph",
    "outputId": "f118ac25-627d-411d-a2c5-465293b88f96"
   },
   "outputs": [],
   "source": [
    "# Model Diagnostics\n",
    "# Variance Inflation Factor (VIF) Analysis\n",
    "# Import necessary library for VIF\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "X_train_df = pd.DataFrame(X_train_scaled, columns=X.columns)\n",
    "\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['Feature'] = X_train_df.columns\n",
    "vif_data['VIF'] = [variance_inflation_factor(X_train_df.values, i) for i in range(X_train_df.shape[1])]\n",
    "vif_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "RRkJZ_TPTjJH",
    "outputId": "8df74948-c117-4bd2-d7eb-86a2d4214ddc"
   },
   "outputs": [],
   "source": [
    "# Model Diagnostics\n",
    "# Residuals vs Fitted Plot (Homoscedasticity Check)\n",
    "# Residuals Analysis\n",
    "residuals = ols_model.resid\n",
    "\n",
    "# Plot residuals vs fitted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=ols_model.fittedvalues, y=residuals, color='blue', alpha=0.7)\n",
    "plt.axhline(0, color='red', linestyle='--', linewidth=1)\n",
    "plt.title(\"Residuals vs Fitted Values\")\n",
    "plt.xlabel(\"Fitted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "zAhaPP87V-yN",
    "outputId": "6447c7d3-77f0-47b3-91d4-14301272fa5d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Model Diagnostics\n",
    "# Q-Q Plot (Normality Check)\n",
    "# Q-Q plot of residuals\n",
    "sm.qqplot(residuals, line=\"45\", fit=True)\n",
    "plt.title(\"Q-Q Plot of Residuals\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "91BsPBSDTjMQ",
    "outputId": "e5841d04-8328-4597-be1c-b696620035f2"
   },
   "outputs": [],
   "source": [
    "# Model Diagnostics\n",
    "# Shapiro-Wilk Test Results with Conditional Message\n",
    "\n",
    "from scipy.stats import shapiro  # Import the Shapiro-Wilk test function\n",
    "\n",
    "# Perform Shapiro-Wilk test\n",
    "shapiro_test = shapiro(residuals)\n",
    "\n",
    "# Print results with conditional interpretation\n",
    "print(\"### Shapiro-Wilk Test Results ###\")\n",
    "print(f\"Test Statistic: {shapiro_test.statistic:.4f}\")\n",
    "print(f\"p-value: {shapiro_test.pvalue:.4e}\")\n",
    "\n",
    "# Conditional message\n",
    "if shapiro_test.pvalue < 0.05:\n",
    "    print(\"\\nMessage: The residuals deviate significantly from normality. Consider applying transformations \"\n",
    "          \"(e.g., log or square root) or exploring non-linear regression techniques.\")\n",
    "else:\n",
    "    print(\"\\nMessage: The residuals do not significantly deviate from normality. The normality assumption is satisfied.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "HdL5b1miTjP5",
    "outputId": "d74961ee-407c-4724-c93c-06eb25d609fe",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Model Diagnostics\n",
    "# Leverage vs Residuals Analysis\n",
    "# Leverage vs Residuals Plot\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sm.graphics.plot_leverage_resid2(ols_model, ax=ax, color='orange')\n",
    "plt.title('Leverage vs Residuals (Refined Log-Transformed Model)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "V-GlRSRSWzZP",
    "outputId": "c4245497-aeb2-4939-e7a8-0f35df2fb72d"
   },
   "outputs": [],
   "source": [
    "# Model Diagnostics\n",
    "# Cook's Distance Plot\n",
    "# Calculate Cook's Distance manually\n",
    "influence = ols_model.get_influence()\n",
    "cooks_d = influence.cooks_distance[0]\n",
    "\n",
    "# Create the custom Cook's Distance plot with orange points\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "# Remove 'use_line_collection=True' since it is not available in older versions.\n",
    "# Instead we plot the stem lines and markers with respective colors\n",
    "markerline, stemlines, baseline = plt.stem(range(len(cooks_d)), cooks_d, markerfmt=\",\", basefmt=\" \")\n",
    "plt.setp(stemlines, 'color', 'orange')  # set stem lines color to orange\n",
    "plt.setp(markerline, 'color', 'orange') # set marker line color to orange\n",
    "\n",
    "\n",
    "plt.axhline(4 / len(X_train), color='red', linestyle='--', label=\"Threshold (4/n)\")\n",
    "plt.title(\"Cook's Distance Plot (Orange Color)\")\n",
    "plt.xlabel(\"Observation Index\")\n",
    "plt.ylabel(\"Cook's Distance\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "06lwpsghXi2K",
    "outputId": "b329887f-4091-4625-fb18-076a83eca563"
   },
   "outputs": [],
   "source": [
    "# Evaluate Test Set Performance\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import numpy as np\n",
    "X_test_with_constant = sm.add_constant(X_test_scaled)\n",
    "y_test_pred = ols_model.predict(X_test_with_constant)\n",
    "\n",
    "# Calculate R^2 and RMSE for the test set\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "\n",
    "# Display evaluation metrics\n",
    "test_performance = {\n",
    "    \"Test R^2\": test_r2,\n",
    "    \"Test RMSE\": test_rmse\n",
    "}\n",
    "\n",
    "test_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Model Testing**\n",
    "\n",
    "Testing differen Models and fine tuning them\n",
    "1. **Simple Linear Regression**\n",
    "2. **Decision Tree**\n",
    "3. **Random Forest**\n",
    "4. **XGBoost**\n",
    "5. **SVM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Simple Linear Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize the Linear Regression model\n",
    "lr_model = LinearRegression()\n",
    "\n",
    "# Train the model\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions and Metrics\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "mse_lr = mean_squared_error(y_test, y_pred_lr)\n",
    "mae_lr = mean_absolute_error(y_test, y_pred_lr)\n",
    "r2_lr = r2_score(y_test, y_pred_lr)\n",
    "\n",
    "print(f\"Linear Regression - Mean Squared Error (MSE): {mse_lr}\")\n",
    "print(f\"Linear Regression - Mean Absolute Error (MAE): {mae_lr}\")\n",
    "print(f\"Linear Regression - R-squared (R²): {r2_lr}\")\n",
    "\n",
    "# Feature Importances (For Linear Regression, coefficients represent feature importance)\n",
    "lr_feature_importances = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': lr_model.coef_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"Top 10 Feature Importances for Linear Regression:\")\n",
    "print(lr_feature_importances.head(10))\n",
    "\n",
    "# Plot Actual vs Predicted Prices\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_test.index, y_test, label='Actual Prices', linestyle='-', marker='o', color='blue')\n",
    "plt.plot(y_test.index, y_pred_lr, label='Predicted Prices', linestyle='--', marker='x', color='orange')\n",
    "plt.fill_between(y_test.index, y_pred_lr - mse_lr**0.5, y_pred_lr + mse_lr**0.5, color='green', alpha=0.2, label='Confidence Interval')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Day-Ahead Price (EUR)')\n",
    "plt.title('Actual vs Predicted Day-Ahead Prices with Linear Regression')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Decision Tree**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fine Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Define the base parameters and tune min_samples_split and min_samples_leaf\n",
    "param_grid_1 = {\n",
    "    'min_samples_split': [3, 4],  # Values to test for min_samples_split\n",
    "    'min_samples_leaf': [6, 8, 10]  # Values to test for min_samples_leaf\n",
    "}\n",
    "\n",
    "base_params = {\n",
    "    'max_depth': 30,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "#Perform Grid Search\n",
    "grid_search_1 = GridSearchCV(\n",
    "    estimator=DecisionTreeRegressor(**base_params),\n",
    "    param_grid=param_grid_1,\n",
    "    cv=3,  # Cross-validation folds\n",
    "    verbose=2,  # Display progress during search\n",
    "    n_jobs=-1  # Use all available processors\n",
    ")\n",
    "\n",
    "#Fit the grid search with training data\n",
    "grid_search_1.fit(X_train, y_train)\n",
    "\n",
    "#Get the best parameters and combine them with the base parameters\n",
    "best_params_final = {**base_params, **grid_search_1.best_params_}\n",
    "\n",
    "# Output best parameters\n",
    "print(f\"Best parameters after tuning min_samples_split and min_samples_leaf: {best_params_final}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Test same parameters more finely based on initial best parameters found previously for min_samples_split and min_samples_leaf\n",
    "param_grid_2 = {\n",
    "    'min_samples_split': [2, 3],\n",
    "    'min_samples_leaf': [15, 20, 25] \n",
    "}\n",
    "\n",
    "#Initialise the Decision Tree with the base parameters\n",
    "base_params = {\n",
    "    'max_depth': 30,  # Example of a parameter from Step 1\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "#Perform Grid Search\n",
    "grid_search_2 = GridSearchCV(\n",
    "    estimator=DecisionTreeRegressor(**base_params),\n",
    "    param_grid=param_grid_2,\n",
    "    cv=3,  # Cross-validation folds\n",
    "    verbose=2,  # Display progress during search\n",
    "    n_jobs=-1  # Use all available processors\n",
    ")\n",
    "\n",
    "#Fit the grid search with training data\n",
    "grid_search_2.fit(X_train, y_train)\n",
    "\n",
    "#Get the best parameters and combine them with the base parameters\n",
    "best_params_final = {**base_params, **grid_search_2.best_params_}\n",
    "\n",
    "#Output best parameters\n",
    "print(f\"Best parameters after tuning min_samples_split and min_samples_leaf: {best_params_final}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Define the max_depth values to test\n",
    "param_grid_3 = {\n",
    "    'max_depth': [5, 10, 15, 20, 25, 30, None]\n",
    "}\n",
    "\n",
    "#Initialise the Decision Tree with the base parameters and teh bezt valuse found for min_samples_leaf and min_samples_split\n",
    "base_params = {\n",
    "    'max_depth': 30, \n",
    "    'random_state': 42,\n",
    "    \"min_samples_leaf\": 20,\n",
    "    'min_samples_split': 2\n",
    "    \n",
    "}\n",
    "\n",
    "#Perform Grid Search\n",
    "grid_search_3 = GridSearchCV(\n",
    "    estimator=DecisionTreeRegressor(**base_params),\n",
    "    param_grid=param_grid_3,\n",
    "    cv=3,  \n",
    "    verbose=2,  \n",
    "    n_jobs=-1  \n",
    ")\n",
    "\n",
    "#Fit the grid search with training data\n",
    "grid_search_3.fit(X_train, y_train)\n",
    "\n",
    "#Get the best parameters and combine them with the base parameters\n",
    "best_params_final = {**base_params, **grid_search_3.best_params_}\n",
    "\n",
    "#Output best parameters\n",
    "print(f\"Best parameters after tuning min_samples_split and min_samples_leaf: {best_params_final}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Test more values of max_depth based on previous best parameter \n",
    "param_grid_4 = {\n",
    "    'max_depth': [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]  # Values to test for min_samples_split\n",
    "}\n",
    "\n",
    "base_params = {\n",
    "    'max_depth': 30,\n",
    "    'random_state': 42,\n",
    "    \"min_samples_leaf\": 20,\n",
    "    'min_samples_split': 2\n",
    "    \n",
    "}\n",
    "\n",
    "#Perform Grid Search\n",
    "grid_search_4 = GridSearchCV(\n",
    "    estimator=DecisionTreeRegressor(**base_params),\n",
    "    param_grid=param_grid_4,\n",
    "    cv=3,  # Cross-validation folds\n",
    "    verbose=2,  # Display progress during search\n",
    "    n_jobs=-1  # Use all available processors\n",
    ")\n",
    "\n",
    "#Fit the grid search with training data\n",
    "grid_search_4.fit(X_train, y_train)\n",
    "\n",
    "#Get the best parameters and combine them with the base parameters\n",
    "best_params_final = {**base_params, **grid_search_4.best_params_}\n",
    "\n",
    "# Output best parameters\n",
    "print(f\"Best parameters after tuning min_samples_split and min_samples_leaf: {best_params_final}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Metrics**  \n",
    "\n",
    "Testing the performance of the model\n",
    "\n",
    "Finding Most Important Features\n",
    "\n",
    "plotting Predicted Vs Actual Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Step 1: Initialise the Decision Tree Regressor with the best parameters\n",
    "best_params = {\n",
    "    'min_samples_leaf': 20,\n",
    "    'min_samples_split': 2,\n",
    "    'max_depth': 11\n",
    "}\n",
    "\n",
    "dt_model = DecisionTreeRegressor(\n",
    "    min_samples_leaf=best_params['min_samples_leaf'],\n",
    "    min_samples_split=best_params['min_samples_split'],\n",
    "    max_depth=best_params['max_depth'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Step 2: Fit the model to your training data\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 3: Predict on the test data\n",
    "dt_y_pred = dt_model.predict(X_test)\n",
    "\n",
    "# Step 4: Calculate MAE, MSE, and R² for Decision Tree\n",
    "dt_mae = mean_absolute_error(y_test, dt_y_pred)\n",
    "dt_mse = mean_squared_error(y_test, dt_y_pred)\n",
    "dt_r2 = r2_score(y_test, dt_y_pred)\n",
    "\n",
    "# Calculate Adjusted R² for Decision Tree\n",
    "n = len(y_test)  # Number of data points\n",
    "p = X_test.shape[1]  # Number of features\n",
    "dt_r2_adj = 1 - ((1 - dt_r2) * (n - 1)) / (n - p - 1)\n",
    "\n",
    "# Step 5: Print the results for Decision Tree\n",
    "print(f\"Decision Tree - Mean Absolute Error (MAE): {dt_mae}\")\n",
    "print(f\"Decision Tree - Mean Squared Error (MSE): {dt_mse}\")\n",
    "print(f\"Decision Tree - R² Score: {dt_r2}\")\n",
    "print(f\"Decision Tree - Adjusted R²: {dt_r2_adj}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importances\n",
    "dt_feature_importances = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': dt_model.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"Top 10 Feature Importances (Decision Tree):\")\n",
    "print(dt_feature_importances.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Predict using the Decision Tree model\n",
    "dt_y_pred = dt_model.predict(X_test)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) for confidence interval\n",
    "dt_mse = mean_squared_error(y_test, dt_y_pred)\n",
    "\n",
    "# Plot Actual vs Predicted Prices\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_test.index, y_test, label='Actual Prices', linestyle='-', marker='o', color='blue')\n",
    "plt.plot(y_test.index, dt_y_pred, label='Predicted Prices', linestyle='--', marker='x', color='orange')\n",
    "\n",
    "# Calculate confidence interval based on MSE\n",
    "confidence_interval_upper = dt_y_pred + (dt_mse ** 0.5)\n",
    "confidence_interval_lower = dt_y_pred - (dt_mse ** 0.5)\n",
    "\n",
    "# Plot Confidence Interval\n",
    "plt.fill_between(y_test.index, confidence_interval_lower, confidence_interval_upper, color='green', alpha=0.2, label='Confidence Interval')\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Day-Ahead Price (EUR)')\n",
    "plt.title('Actual vs Predicted Day-Ahead Prices with Decision Tree')\n",
    "\n",
    "# Add legend and grid\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Random Forest**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fine Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Testing different values for min_samples_split and min_samples_leaf\n",
    "param_grid_1 = {\n",
    "    'min_samples_split': [3, 5, 7],\n",
    "    'min_samples_leaf': [4, 5, 6]\n",
    "}\n",
    "\n",
    "#initialise the Tree parameters\n",
    "best_params = {\n",
    "    'n_estimators': 200,\n",
    "    'max_depth': 30,\n",
    "    'bootstrap': True\n",
    "}\n",
    "\n",
    "\n",
    "#Perform Grid Search\n",
    "grid_search_1 = GridSearchCV(\n",
    "    estimator=RandomForestRegressor(\n",
    "        n_estimators=best_params['n_estimators'],  # Using best_params['n_estimators']\n",
    "        max_depth=best_params['max_depth'],  # Using best_params['max_depth']\n",
    "        bootstrap=best_params['bootstrap'],  # Using best_params['bootstrap']\n",
    "        random_state=42\n",
    "    ),\n",
    "    param_grid=param_grid_1,\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "#Fit the grid search with training data\n",
    "grid_search_1.fit(X_train, y_train)\n",
    "\n",
    "#print best parameters\n",
    "best_params_final = {**best_params, **grid_search_1.best_params_}\n",
    "print(f\"Best parameters after second round: {best_params_final}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing more values for tune min_samples_split and min_samples_leaf based on previous results\n",
    "param_grid_2 = {\n",
    "    'min_samples_split': [3, 4],\n",
    "    'min_samples_leaf': [6, 8, 10]\n",
    "}\n",
    "\n",
    "#initialise the Tree parameters\n",
    "best_params = {\n",
    "    'n_estimators': 200,\n",
    "    'max_depth': 30,\n",
    "    'bootstrap': True\n",
    "}\n",
    "\n",
    "#Perform Grid Search\n",
    "grid_search_2 = GridSearchCV(\n",
    "    estimator=RandomForestRegressor(\n",
    "        n_estimators=best_params['n_estimators'],  \n",
    "        max_depth=best_params['max_depth'], \n",
    "        bootstrap=best_params['bootstrap'],\n",
    "        random_state=42\n",
    "    ),\n",
    "    param_grid=param_grid_2,\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "#Fit the grid search with training data\n",
    "grid_search_2.fit(X_train, y_train)\n",
    "\n",
    "#print best parameters\n",
    "best_params_final = {**best_params, **grid_search_2.best_params_}\n",
    "print(f\"Best parameters after second round: {best_params_final}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the best parameter found for min_samples_split and testing more min_samples_leaf\n",
    "param_grid_3 = {\n",
    "    'min_samples_leaf': [10, 20, 50]\n",
    "}\n",
    "\n",
    "#Initialising the base parameters\n",
    "best_params = {\n",
    "    'n_estimators': 200,\n",
    "    'max_depth': 30,\n",
    "    'min_samples_split': 3, #Keeping this value to avoid overfitting\n",
    "    'bootstrap': True\n",
    "}\n",
    "\n",
    "#Perform Grid Search\n",
    "grid_search_3 = GridSearchCV(\n",
    "    estimator=RandomForestRegressor(\n",
    "        min_samples_split=best_params['min_samples_split'],  # Using best_params['n_estimators']\n",
    "        n_estimators=best_params['n_estimators'],  # Using best_params['n_estimators']\n",
    "        max_depth=best_params['max_depth'],  # Using best_params['max_depth']\n",
    "        bootstrap=best_params['bootstrap'],  # Using best_params['bootstrap']\n",
    "        random_state=42\n",
    "    ),\n",
    "    param_grid=param_grid_3,\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "#Fit the grid search with training data\n",
    "grid_search_3.fit(X_train, y_train)\n",
    "\n",
    "#print best parameters\n",
    "best_params_final = {**best_params, **grid_search_3.best_params_}\n",
    "print(f\"Best parameters after second round: {best_params_final}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_4 = {\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "#Initialising the base parameters\n",
    "best_params = {\n",
    "    'n_estimators': 200,\n",
    "    'max_depth': 30,\n",
    "    'min_samples_split': 3,\n",
    "    'min_samples_leaf': 10\n",
    "}\n",
    "\n",
    "grid_search_4 = GridSearchCV(\n",
    "    estimator=RandomForestRegressor(\n",
    "        min_samples_split=3,  \n",
    "        n_estimators=200,  \n",
    "        max_depth=30,  \n",
    "        min_samples_leaf = 10,\n",
    "        random_state=42\n",
    "    ),\n",
    "    param_grid=param_grid_4,\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the grid search with training data\n",
    "grid_search_4.fit(X_train, y_train)\n",
    "\n",
    "# Final best parameters after the second round of tuning\n",
    "best_params_final = {**best_params, **grid_search_4.best_params_}\n",
    "print(f\"Best parameters after second round: {best_params_final}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing different values for n_estimators and max_depth\n",
    "param_grid_5 = {\n",
    "    'n_estimators': [200, 250, 300],  # Different values for n_estimators\n",
    "    'max_depth': [30, 40, 50]   # Different values for max_depth\n",
    "}\n",
    "\n",
    "#Initialising with base parameters based on best parameters found so far\n",
    "best_params = {\n",
    "    'min_samples_split': 3,\n",
    "    'min_samples_leaf': 10,\n",
    "    'bootstrap': True\n",
    "}\n",
    "\n",
    "#Perform Grid Search\n",
    "grid_search_5 = GridSearchCV(\n",
    "    estimator=RandomForestRegressor(\n",
    "        min_samples_split=3,   \n",
    "        min_samples_leaf = 10,\n",
    "        bootstrap = True,\n",
    "        random_state=42\n",
    "    ),\n",
    "    param_grid=param_grid_5,\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "#Fit the grid search with training data\n",
    "grid_search_5.fit(X_train, y_train)\n",
    "\n",
    "#Final best parameters after the second round of tuning\n",
    "best_params_final = {**best_params, **grid_search_5.best_params_}\n",
    "print(f\"Best parameters after second round: {best_params_final}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_6 = {\n",
    "    'n_estimators': [300, 250, 400]\n",
    "}\n",
    "\n",
    "best_params = {\n",
    "    'max_depth': 30,\n",
    "    'min_samples_split': 3,\n",
    "    'min_samples_leaf': 10,\n",
    "    'bootstrap': True\n",
    "}\n",
    "\n",
    "\n",
    "grid_search_6 = GridSearchCV(\n",
    "    estimator=RandomForestRegressor(\n",
    "        min_samples_split=3,   \n",
    "        min_samples_leaf = 10,\n",
    "        bootstrap = True,\n",
    "        max_depth = 30,\n",
    "        random_state=42\n",
    "    ),\n",
    "    param_grid=param_grid_6,\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "#Fit the grid search with training data\n",
    "grid_search_6.fit(X_train, y_train)\n",
    "\n",
    "#Final best parameters after the second round of tuning\n",
    "best_params_final = {**best_params, **grid_search_6.best_params_}\n",
    "print(f\"Best parameters after second round: {best_params_final}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_7 = {\n",
    "    'n_estimators': [500, 550, 600]\n",
    "}\n",
    "\n",
    "best_params = {\n",
    "    'max_depth': 30,\n",
    "    'min_samples_split': 3,\n",
    "    'min_samples_leaf': 10,\n",
    "    'bootstrap': True\n",
    "}\n",
    "\n",
    "grid_search_7 = GridSearchCV(\n",
    "    estimator=RandomForestRegressor(\n",
    "        min_samples_split=3,   \n",
    "        min_samples_leaf = 10,\n",
    "        bootstrap = True,\n",
    "        max_depth = 30,\n",
    "        random_state=42\n",
    "    ),\n",
    "    param_grid=param_grid_7,\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "#Fit the grid search with training data\n",
    "grid_search_7.fit(X_train, y_train)\n",
    "\n",
    "#Final best parameters after the second round of tuning\n",
    "best_params_final = {**best_params, **grid_search_7.best_params_}\n",
    "print(f\"Best parameters after second round: {best_params_final}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Metrics**  \n",
    "\n",
    "Testing the performance of the model\n",
    "\n",
    "Finding Most Important Features\n",
    "\n",
    "plotting Predicted Vs Actual Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "#Define the best parameters found from GridSearchCV or RandomizedSearchCV\n",
    "rf_best_params = {\n",
    "    'n_estimators': 500,\n",
    "    'max_depth': 30,\n",
    "    'min_samples_split': 3,\n",
    "    'min_samples_leaf': 10,\n",
    "    'bootstrap': True\n",
    "}\n",
    "\n",
    "#Initialise the RandomForestRegressor with the best parameters\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=rf_best_params['n_estimators'],\n",
    "    max_depth=rf_best_params['max_depth'],\n",
    "    min_samples_split=rf_best_params['min_samples_split'],\n",
    "    min_samples_leaf=rf_best_params['min_samples_leaf'],\n",
    "    bootstrap=rf_best_params['bootstrap'],\n",
    "    random_state=42  # Optional: To ensure reproducibility\n",
    ")\n",
    "\n",
    "#Fit the model on the training data\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "#Predict on the test data (optional, for evaluation)\n",
    "rf_y_pred = rf_model.predict(X_test)\n",
    "\n",
    "#Evaluate the model's performance (optional)\n",
    "rf_mse = mean_squared_error(y_test, rf_y_pred)\n",
    "rf_mae = mean_absolute_error(y_test, rf_y_pred)\n",
    "rf_r2 = r2_score(y_test, rf_y_pred)\n",
    "\n",
    "#Calculate Adjusted R² for Random Forest\n",
    "n = len(y_test)  # Number of data points\n",
    "p = X_test.shape[1]  # Number of features\n",
    "rf_r2_adj = 1 - ((1 - rf_r2) * (n - 1)) / (n - p - 1)\n",
    "\n",
    "#Print the results for Random Forest\n",
    "print(f\"Random Forest - Mean Squared Error (MSE): {rf_mse}\")\n",
    "print(f\"Random Forest - Mean Absolute Error (MAE): {rf_mae}\")\n",
    "print(f\"Random Forest - R² Score: {rf_r2}\")\n",
    "print(f\"Random Forest - Adjusted R²: {rf_r2_adj}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Importances for Random Forest\n",
    "rf_feature_importances = pd.DataFrame({\n",
    "    'Feature': X_train.columns,  # Ensure X_train is a DataFrame with column names\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "#Display the top 10 features\n",
    "print(\"Top 10 Feature Importances:\")\n",
    "print(rf_feature_importances.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#Predict using the Random Forest model\n",
    "rf_y_pred = rf_model.predict(X_test)\n",
    "\n",
    "#Plot Actual vs Predicted Prices\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_test.index, y_test, label='Actual Prices', linestyle='-', marker='o', color='blue')\n",
    "plt.plot(y_test.index, rf_y_pred, label='Predicted Prices', linestyle='--', marker='x', color='orange')\n",
    "\n",
    "#Calculate confidence interval based on MSE\n",
    "confidence_interval_upper = rf_y_pred + (rf_mse ** 0.5)\n",
    "confidence_interval_lower = rf_y_pred - (rf_mse ** 0.5)\n",
    "\n",
    "#Plot Confidence Interval\n",
    "plt.fill_between(y_test.index, confidence_interval_lower, confidence_interval_upper, color='green', alpha=0.2, label='Confidence Interval')\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Day-Ahead Price (EUR)')\n",
    "plt.title('Actual vs Predicted Day-Ahead Prices with Random Forest')\n",
    "\n",
    "#Add legend and grid\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "#Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **XGBoost**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fine Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import xgboost as xgb\n",
    "\n",
    "#Initialise the model with fixed parameters\n",
    "fixed_params = {\n",
    "    'learning_rate': 0.1,  # Fixed\n",
    "    'n_estimators': 1000,  # Fixed\n",
    "    'max_depth': 5,  # Fixed\n",
    "    'min_child_weight': 1,  # Fixed\n",
    "    'gamma': 0,  # Fixed\n",
    "    'subsample': 0.8,  # Fixed\n",
    "    'colsample_bytree': 0.8,  # Fixed\n",
    "    'nthread': 4,  # Fixed\n",
    "    'scale_pos_weight': 1,  # Fixed\n",
    "    'seed': 27  # Fixed\n",
    "}\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(**fixed_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid for tuning max_depth and min_child_weight\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'min_child_weight': [1, 3, 5]\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,  # Cross-validation\n",
    "    verbose=2,  # Detailed progress messages\n",
    "    n_jobs=-1  # Use all available processors\n",
    ")\n",
    "\n",
    "# Fit the model on the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model and its parameters\n",
    "best_model = grid_search.best_estimator_\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for tuning max_depth and min_child_weight\n",
    "param_grid = {\n",
    "    'max_depth': [2, 3, 4],\n",
    "    'min_child_weight': [1, 2]\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,  # Cross-validation\n",
    "    verbose=2,  # Detailed progress messages\n",
    "    n_jobs=-1  # Use all available processors\n",
    ")\n",
    "\n",
    "# Fit the model on the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model and its parameters\n",
    "best_model = grid_search.best_estimator_\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import xgboost as xgb\n",
    "\n",
    "#Initialize the model with fixed parameters\n",
    "fixed_params = {\n",
    "    'learning_rate': 0.1,  # Fixed\n",
    "    'n_estimators': 1000,  # Fixed\n",
    "    'max_depth': 3,  # Fixed\n",
    "    'min_child_weight': 2,  # Fixed\n",
    "    'gamma': 0,  # Fixed\n",
    "    'subsample': 0.8,  # Fixed\n",
    "    'colsample_bytree': 0.8,  # Fixed\n",
    "    'nthread': 4,  # Fixed\n",
    "    'scale_pos_weight': 1,  # Fixed\n",
    "    'seed': 27  # Fixed\n",
    "}\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(**fixed_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Define the parameter grid for gamma\n",
    "param_test = {\n",
    "    'gamma': [i/10.0 for i in range(0, 5)]  # Testing values: [0.0, 0.1, 0.2, 0.3, 0.4]\n",
    "}\n",
    "\n",
    "#Perform GridSearchCV with XGBRegressor and your defined parameter grid\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_model,  # xgb_model should already be initialized with your fixed parameters\n",
    "    param_grid=param_test,\n",
    "    cv=3,  # Cross-validation\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "#Fit the model to your training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "#Get the best model and its parameters\n",
    "best_model = grid_search.best_estimator_\n",
    "print(f\"Best parameters for gamma: {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import xgboost as xgb\n",
    "\n",
    "#Initialise the model with fixed parameters\n",
    "fixed_params = {\n",
    "    'learning_rate': 0.1,  # Fixed\n",
    "    'n_estimators': 1000,  # Fixed\n",
    "    'max_depth': 3,  # Fixed\n",
    "    'min_child_weight': 2,  # Fixed\n",
    "    'gamma': 0.3,  # Fixed\n",
    "    'subsample': 0.8,  # Fixed\n",
    "    'colsample_bytree': 0.8,  # Fixed\n",
    "    'nthread': 4,  # Fixed\n",
    "    'scale_pos_weight': 1,  # Fixed\n",
    "    'seed': 27  # Fixed\n",
    "}\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(**fixed_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test = {\n",
    "    'subsample': [i/10.0 for i in range(6, 10)],  # [0.6, 0.7, 0.8, 0.9]\n",
    "    'colsample_bytree': [i/10.0 for i in range(6, 10)]  # [0.6, 0.7, 0.8, 0.9]\n",
    "}\n",
    "\n",
    "#Perform GridSearchCV with XGBRegressor and your defined parameter grid\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_model,  # xgb_model should already be initialized with your fixed parameters\n",
    "    param_grid=param_test,\n",
    "    cv=3,  # Cross-validation\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "#Fit the model to your training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "#Get the best model and its parameters\n",
    "best_model = grid_search.best_estimator_\n",
    "print(f\"Best parameters for subsample and colsample_bytree: {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test = {\n",
    " 'subsample':[i/100.0 for i in range(75,90,5)],\n",
    " 'colsample_bytree':[i/100.0 for i in range(75,90,5)]\n",
    "}\n",
    "\n",
    "#Perform GridSearchCV with XGBRegressor and your defined parameter grid\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_model,  # xgb_model should already be initialized with your fixed parameters\n",
    "    param_grid=param_test,\n",
    "    cv=3,  # Cross-validation\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "#Fit the model to your training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "#Get the best model and its parameters\n",
    "best_model = grid_search.best_estimator_\n",
    "print(f\"Best parameters for subsample and colsample_bytree: {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import xgboost as xgb\n",
    "\n",
    "#Initialise the model with fixed parameters\n",
    "fixed_params = {\n",
    "    'learning_rate': 0.1,  # Fixed\n",
    "    'n_estimators': 1000,  # Fixed\n",
    "    'max_depth': 3,  # Fixed\n",
    "    'min_child_weight': 2,  # Fixed\n",
    "    'gamma': 0.3,  # Fixed\n",
    "    'subsample': 0.8,  # Fixed\n",
    "    'colsample_bytree': 0.8,  # Fixed\n",
    "    'nthread': 4,  # Fixed\n",
    "    'scale_pos_weight': 1,  # Fixed\n",
    "    'seed': 27  # Fixed\n",
    "}\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(**fixed_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test = {\n",
    "    'reg_alpha': [1e-5, 1e-2, 0.1, 1, 100]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_model,  # xgb_model should already be initialized with your fixed parameters\n",
    "    param_grid=param_test,\n",
    "    cv=3,  # Cross-validation\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "#Fit the model to your training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "#Get the best model and its parameters\n",
    "best_model = grid_search.best_estimator_\n",
    "print(f\"Best parameters for subsample and colsample_bytree: {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test = {\n",
    "    'reg_alpha': [10, 50, 75, 100, 125, 150, 175]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_model,  # xgb_model should already be initialized with your fixed parameters\n",
    "    param_grid=param_test,\n",
    "    cv=3,  # Cross-validation\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "#Fit the model to your training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "#Get the best model and its parameters\n",
    "best_model = grid_search.best_estimator_\n",
    "print(f\"Best parameters for subsample and colsample_bytree: {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test = {\n",
    "    'reg_alpha': [115, 120, 125]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_model,  # xgb_model should already be initialized with your fixed parameters\n",
    "    param_grid=param_test,\n",
    "    cv=3,  # Cross-validation\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "#Fit the model to your training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "#Get the best model and its parameters\n",
    "best_model = grid_search.best_estimator_\n",
    "print(f\"Best parameters for subsample and colsample_bytree: {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Metrics**  \n",
    "\n",
    "Testing the performance of the model\n",
    "\n",
    "Finding Most Important Features\n",
    "\n",
    "plotting Predicted Vs Actual Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import xgboost as xgb\n",
    "\n",
    "# Define the best parameters found from your GridSearchCV\n",
    "xgb_best_params = {\n",
    "    'reg_alpha': 120,  # Best reg_alpha from grid search\n",
    "    'max_depth': 3,  # Best max_depth from grid search\n",
    "    'min_child_weight': 2,  # Best min_child_weight from grid search\n",
    "    'gamma': 0.3,  # Best gamma from grid search\n",
    "    'colsample_bytree': 0.8,  # Best colsample_bytree from grid search\n",
    "    'subsample': 0.8  # Best subsample from grid search\n",
    "}\n",
    "\n",
    "#Initialise XGBoost model with the best parameters\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_estimators=5000,  # You can adjust this depending on your model tuning\n",
    "    random_state=42,\n",
    "    learning_rate=0.01,  # Best parameter for learning_rate\n",
    "    reg_alpha=xgb_best_params['reg_alpha'],  # Add the best reg_alpha\n",
    "    max_depth=xgb_best_params['max_depth'],  # Add the best max_depth\n",
    "    min_child_weight=xgb_best_params['min_child_weight'],  # Add the best min_child_weight\n",
    "    gamma=xgb_best_params['gamma'],  # Add the best gamma\n",
    "    colsample_bytree=xgb_best_params['colsample_bytree'],  # Add the best colsample_bytree\n",
    "    subsample=xgb_best_params['subsample']  # Add the best subsample\n",
    ")\n",
    "\n",
    "#Train the model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "#Calculate performance metrics\n",
    "xgb_mse = mean_squared_error(y_test, xgb_y_pred)\n",
    "xgb_mae = mean_absolute_error(y_test, xgb_y_pred)\n",
    "xgb_r2 = r2_score(y_test, xgb_y_pred)\n",
    "\n",
    "#Calculate Adjusted R²\n",
    "n = len(y_test)  # Number of data points\n",
    "p = X_test.shape[1]  # Number of features\n",
    "xgb_r2_adj = 1 - ((1 - xgb_r2) * (n - 1)) / (n - p - 1)\n",
    "\n",
    "#Print the results\n",
    "print(f\"XGBoost - Mean Squared Error (MSE): {xgb_mse}\")\n",
    "print(f\"XGBoost - Mean Absolute Error (MAE): {xgb_mae}\")\n",
    "print(f\"XGBoost - R² Score: {xgb_r2}\")\n",
    "print(f\"XGBoost - Adjusted R²: {xgb_r2_adj}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importances\n",
    "xgb_feature_importances = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': xgb_model.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"Top 10 Feature Importances:\")\n",
    "print(xgb_feature_importances.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Plot Actual vs Predicted Prices\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_test.index, y_test, label='Actual Prices', linestyle='-', marker='o', color='blue')\n",
    "plt.plot(y_test.index, xgb_y_pred, label='Predicted Prices', linestyle='--', marker='x', color='orange')\n",
    "\n",
    "# Calculate confidence interval based on MSE\n",
    "confidence_interval_upper = xgb_y_pred + (xgb_mse ** 0.5)\n",
    "confidence_interval_lower = xgb_y_pred - (xgb_mse ** 0.5)\n",
    "\n",
    "# Plot Confidence Interval\n",
    "plt.fill_between(y_test.index, confidence_interval_lower, confidence_interval_upper, color='green', alpha=0.2, label='Confidence Interval')\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Day-Ahead Price (EUR)')\n",
    "plt.title('Actual vs Predicted Day-Ahead Prices with XGBoost')\n",
    "\n",
    "# Add legend and grid\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Ensure your training and testing data are already defined (X_train, X_test, y_train, y_test)\n",
    "\n",
    "# 1.Scaling the Data: Standardize features for better performance with SVM.\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 2.Train the Support Vector Machine (SVM) model\n",
    "svm_model = SVR(kernel='rbf')  # You can change the kernel if needed (e.g., 'linear', 'poly', 'rbf')\n",
    "svm_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 3.Make predictions using the trained model\n",
    "y_pred_svm = svm_model.predict(X_test_scaled)\n",
    "\n",
    "# 4.Calculate Mean Squared Error (MSE)\n",
    "mse_svm = mean_squared_error(y_test, y_pred_svm)\n",
    "\n",
    "# 5.Calculate Mean Absolute Error (MAE)\n",
    "mae_svm = mean_absolute_error(y_test, y_pred_svm)\n",
    "\n",
    "# 6.Calculate R-squared (R²)\n",
    "r2_svm = r2_score(y_test, y_pred_svm)\n",
    "\n",
    "# 7.Calculate Adjusted R-squared\n",
    "n = len(y_test)  # Number of test samples\n",
    "p = X_test.shape[1]  # Number of features\n",
    "adj_r2_svm = 1 - (1 - r2_svm) * (n - 1) / (n - p - 1)\n",
    "\n",
    "#Print results\n",
    "print(f\"SVM - Mean Squared Error (MSE): {mse_svm}\")\n",
    "print(f\"SVM - Mean Absolute Error (MAE): {mae_svm}\")\n",
    "print(f\"SVM - R-squared (R²): {r2_svm}\")\n",
    "print(f\"SVM - Adjusted R-squared (Adjusted R²): {adj_r2_svm}\")\n",
    "\n",
    "\n",
    "print(\"performing cross validation...\")\n",
    "# 8.Cross-validation to evaluate the model performance\n",
    "cv_scores = cross_val_score(svm_model, X_train_scaled, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "print(f\"Cross-Validation MSE: {-cv_scores.mean()} (+/- {cv_scores.std()})\")\n",
    "\n",
    "# 9.Hyperparameter tuning using GridSearchCV\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [0.001, 0.01, 0.1, 1],\n",
    "    'epsilon': [0.01, 0.1, 0.2, 0.3]\n",
    "}\n",
    "print(\"performing grid search...\")\n",
    "grid_search = GridSearchCV(SVR(), param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "\n",
    "# 10.Plot Actual vs Predicted Prices (for visualization)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_test.index, y_test, label='Actual Prices', linestyle='-', marker='o', color='blue')\n",
    "plt.plot(y_test.index, y_pred_svm, label='Predicted Prices', linestyle='--', marker='x', color='orange')\n",
    "plt.fill_between(y_test.index, y_pred_svm - mse_svm**0.5, y_pred_svm + mse_svm**0.5, color='green', alpha=0.2, label='Confidence Interval')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Day-Ahead Price (EUR)')\n",
    "plt.title('Actual vs Predicted Day-Ahead Prices with SVM')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Ensure your training and testing data are already defined (X_train, X_test, y_train, y_test)\n",
    "\n",
    "# 1.Scaling the Data**: Standardize features for better performance with SVM.\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 2.Train the Support Vector Machine (SVM) model\n",
    "svm_model = SVR(kernel='rbf', C=100, epsilon=0.01, gamma=0.001)  # You can change the kernel if needed (e.g., 'linear', 'poly', 'rbf')\n",
    "svm_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 3.Make predictions using the trained model\n",
    "y_pred_svm = svm_model.predict(X_test_scaled)\n",
    "\n",
    "# 4.Calculate Mean Squared Error (MSE)\n",
    "mse_svm = mean_squared_error(y_test, y_pred_svm)\n",
    "\n",
    "# 5.Calculate Mean Absolute Error (MAE)\n",
    "mae_svm = mean_absolute_error(y_test, y_pred_svm)\n",
    "\n",
    "# 6.Calculate R-squared (R²)\n",
    "r2_svm = r2_score(y_test, y_pred_svm)\n",
    "\n",
    "# 7.Calculate Adjusted R-squared\n",
    "n = len(y_test)  # Number of test samples\n",
    "p = X_test.shape[1]  # Number of features\n",
    "adj_r2_svm = 1 - (1 - r2_svm) * (n - 1) / (n - p - 1)\n",
    "\n",
    "# Print results\n",
    "print(f\"SVM - Mean Squared Error (MSE): {mse_svm}\")\n",
    "print(f\"SVM - Mean Absolute Error (MAE): {mae_svm}\")\n",
    "print(f\"SVM - R-squared (R²): {r2_svm}\")\n",
    "print(f\"SVM - Adjusted R-squared (Adjusted R²): {adj_r2_svm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "#Calculate permutation importance\n",
    "result = permutation_importance(svm_model, X_test_scaled, y_test, n_repeats=10, random_state=42)\n",
    "\n",
    "#Organise results into a DataFrame\n",
    "feature_importances = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': result.importances_mean\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "#Display top 10 features\n",
    "print(\"Top Feature Importances:\")\n",
    "print(feature_importances.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_svm = svm_model.predict(X_test_scaled)\n",
    "\n",
    "mse_svm = mean_squared_error(y_test, y_pred_svm)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_test.index, y_test, label='Actual Prices', linestyle='-', marker='o', color='blue')\n",
    "plt.plot(y_test.index, y_pred_svm, label='Predicted Prices', linestyle='--', marker='x', color='orange')\n",
    "\n",
    "confidence_interval_upper = y_pred_svm + (mse_svm ** 0.5)\n",
    "confidence_interval_lower = y_pred_svm - (mse_svm ** 0.5)\n",
    "\n",
    "plt.fill_between(y_test.index, confidence_interval_lower, confidence_interval_upper, color='green', alpha=0.2, label='Confidence Interval')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Day-Ahead Price (EUR)')\n",
    "plt.title('Actual vs Predicted Day-Ahead Prices with SVM')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
